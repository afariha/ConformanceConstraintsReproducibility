%!TEX root = paper.tex
%
\looseness-1 The reliability of inferences made by data-driven systems hinges
on the data's continued conformance to the systems' initial settings and
assumptions. When serving data (on which we want to apply inference) deviates
from the profile of the initial training data, the outcome of inference becomes
unreliable. 
%
We introduce \emph{conformance constraints}, a new data profiling primitive
tailored towards quantifying the degree of \emph{non-conformance}, which can
effectively characterize if inference over that tuple is \emph{untrustworthy}.
%
\Dis are constraints over certain
arithmetic expressions (called \emph{projections}) involving the numerical
attributes of a dataset, which existing data profiling primitives such as
functional dependencies and denial constraints cannot model.
%
Our key finding is that projections that incur \emph{low variance} on a dataset
construct effective \dis. This principle yields the surprising result that
low-variance components of a principal component analysis, which are usually
discarded for dimensionality reduction, generate stronger \dis than the
high-variance components. Based on this result, we provide a highly scalable
and efficient technique---linear in data size and cubic in the number of
attributes---for discovering conformance constraints for a dataset. To measure
the degree of a tuple's non-conformance with respect to a dataset, we propose a
\emph{quantitative semantics} that captures how much a tuple violates the \dis
of that dataset.
%
We demonstrate the value of \dis on two applications: \emph{trusted machine
learning} and \emph{data drift}. We empirically show that \dis offer mechanisms
to (1)~reliably detect tuples on which the inference of a machine-learned model
should not be trusted, and (2)~quantify data drift more accurately than the
state of the art.